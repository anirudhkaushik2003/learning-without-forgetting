{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895121aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.0\n",
      "Torchvision Version:  0.14.0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "from iirc.datasets_loader import get_lifelong_datasets\n",
    "from iirc.definitions import PYTORCH, IIRC_SETUP\n",
    "from iirc.utils.download_cifar import download_extract_cifar100\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54f5158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading CIFAR 100\n",
      "dataset downloaded\n",
      "extracting CIFAR 100\n",
      "dataset extracted\n"
     ]
    }
   ],
   "source": [
    "download_extract_cifar100(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2380c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "essential_transforms_fn = transforms.ToTensor()\n",
    "augmentation_transforms_fn = transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bdb847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iirc_cifar100\n",
      "Setup used: IIRC\n",
      "Using PyTorch\n",
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "dataset_splits, tasks, class_names_to_idx = \\\n",
    "    get_lifelong_datasets(dataset_name = \"iirc_cifar100\",\n",
    "                          dataset_root = \"../../data\", # the imagenet folder (where the train and val folders reside, or the parent directory of cifar-100-python folder\n",
    "                          setup = IIRC_SETUP,\n",
    "                          framework = PYTORCH,\n",
    "                          tasks_configuration_id = 0,\n",
    "                          essential_transforms_fn = essential_transforms_fn,\n",
    "                          augmentation_transforms_fn = augmentation_transforms_fn,\n",
    "                          joint = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0aea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tasks))\n",
    "n_classes_per_task = []\n",
    "for task in tasks:\n",
    "    n_classes_per_task.append(len(task))\n",
    "n_classes_per_task = np.array(n_classes_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f45387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "intask_valid\n",
      "posttask_valid\n",
      "test\n",
      "('small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'small_mammals', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'television', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'mountain', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider', 'spider')\n"
     ]
    }
   ],
   "source": [
    "# lifelong_datasets['train'].choose_task(2)\n",
    "# print(list(zip(*lifelong_datasets['train']))[1])\n",
    "for i in dataset_splits:\n",
    "    print(i)\n",
    "dataset_splits[\"test\"].choose_task(1)\n",
    "print(list(zip(*dataset_splits[\"test\"]))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a pretrained model (imageNet)\n",
    "model_name = \"resnet\" #choosing alexnet since it is \"relatively\" easy to train\n",
    "# model_name = \"squeezenet\" # changed to squeezeNet since it gets same acc as alex but smaller\n",
    "num_classes = 9 # in cifar100\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "feature_extract = False #set to false so we can finetune entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81089e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, criterion, optimizer, num_classes, num_epochs=5 ):\n",
    "    since = time.time() # including this just because\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "                \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # iterate over data\n",
    "        for inputs,label1,label2 in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "            label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "            label1 = label1.to(torch.float32)\n",
    "            label1 = label1.to(device)\n",
    "#             label2 = label2.to(device)\n",
    "\n",
    "\n",
    "            #empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, label1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(\"len dataset = \",len(trainloader.dataset))\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "        print('{} Loss: {:.4f}'.format('train', epoch_loss))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2405003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ae5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.num_ftrs = self.resnet.fc.in_features\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6e52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model, num_classes):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bacd09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MultilabelJaccardIndex\n",
    "def test_model(model,testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    JS = MultilabelJaccardIndex(num_labels = 2)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, label1,label2 = data\n",
    "            label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "            label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "            label1 = label1.to(torch.float32)\n",
    "            \n",
    "            label2 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label2]))\n",
    "            label2 = F.one_hot(label2, num_classes=num_classes)\n",
    "            label2 = label2.to(torch.float32)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            predicted = (outputs.data > 0.5).float()\n",
    "            correct = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8188f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "len dataset =  8160\n",
      "train Loss: 0.2706\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  8160\n",
      "train Loss: 0.2212\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  8160\n",
      "train Loss: 0.1995\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  8160\n",
      "train Loss: 0.1898\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  8160\n",
      "train Loss: 0.1800\n",
      "\n",
      "Training complete in 3m 11s\n",
      "Epoch 1/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.1708\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0874\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0802\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0766\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0748\n",
      "\n",
      "Training complete in 0m 42s\n",
      "Epoch 1/5\n",
      "len dataset =  2160\n",
      "train Loss: 0.1482\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  2160\n",
      "train Loss: 0.0669\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  2160\n",
      "train Loss: 0.0568\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  2160\n",
      "train Loss: 0.0543\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  2160\n",
      "train Loss: 0.0513\n",
      "\n",
      "Training complete in 0m 51s\n",
      "Epoch 1/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.1600\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0604\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0520\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0464\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0472\n",
      "\n",
      "Training complete in 0m 40s\n",
      "Epoch 1/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.1844\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0680\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0552\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0508\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0458\n",
      "\n",
      "Training complete in 0m 37s\n",
      "Epoch 1/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.1701\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0559\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0461\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0414\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0393\n",
      "\n",
      "Training complete in 0m 37s\n",
      "Epoch 1/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.1831\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0618\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0500\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0450\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0410\n",
      "\n",
      "Training complete in 0m 39s\n",
      "Epoch 1/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.1900\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0562\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0433\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0376\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0343\n",
      "\n",
      "Training complete in 0m 39s\n",
      "Epoch 1/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.1934\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0596\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0467\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0401\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0371\n",
      "\n",
      "Training complete in 0m 40s\n",
      "Epoch 1/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.2038\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0602\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0471\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0409\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0375\n",
      "\n",
      "Training complete in 0m 41s\n",
      "Epoch 1/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.1830\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0498\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0378\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0324\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0293\n",
      "\n",
      "Training complete in 0m 46s\n",
      "Epoch 1/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.2360\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0573\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0400\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0336\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0291\n",
      "\n",
      "Training complete in 0m 37s\n",
      "Epoch 1/5\n",
      "len dataset =  2320\n",
      "train Loss: 0.1918\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  2320\n",
      "train Loss: 0.0459\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  2320\n",
      "train Loss: 0.0335\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  2320\n",
      "train Loss: 0.0281\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  2320\n",
      "train Loss: 0.0255\n",
      "\n",
      "Training complete in 0m 53s\n",
      "Epoch 1/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.2189\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0578\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0426\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0363\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1840\n",
      "train Loss: 0.0329\n",
      "\n",
      "Training complete in 0m 43s\n",
      "Epoch 1/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.2443\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0647\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0467\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0388\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0342\n",
      "\n",
      "Training complete in 0m 38s\n",
      "Epoch 1/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.2350\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0616\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0439\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0368\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0323\n",
      "\n",
      "Training complete in 0m 39s\n",
      "Epoch 1/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.2394\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0590\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0411\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0336\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1760\n",
      "train Loss: 0.0294\n",
      "\n",
      "Training complete in 0m 42s\n",
      "Epoch 1/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.2088\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0540\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0384\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0327\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0284\n",
      "\n",
      "Training complete in 0m 46s\n",
      "Epoch 1/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.2450\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0621\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0429\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0348\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0300\n",
      "\n",
      "Training complete in 0m 39s\n",
      "Epoch 1/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.2150\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0539\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0384\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0319\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  2080\n",
      "train Loss: 0.0282\n",
      "\n",
      "Training complete in 0m 47s\n",
      "Epoch 1/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.2388\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0627\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0436\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0361\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1680\n",
      "train Loss: 0.0315\n",
      "\n",
      "Training complete in 0m 40s\n",
      "Epoch 1/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.2629\n",
      "\n",
      "Epoch 2/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0673\n",
      "\n",
      "Epoch 3/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0462\n",
      "\n",
      "Epoch 4/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0379\n",
      "\n",
      "Epoch 5/5\n",
      "len dataset =  1600\n",
      "train Loss: 0.0332\n",
      "\n",
      "Training complete in 0m 38s\n"
     ]
    }
   ],
   "source": [
    "# Setup \n",
    "# BCE loss for multi-label classification\n",
    "# sigmoid activation after FC layer \n",
    "# everything above 0.5 is a predicted label\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# get dataset corresponding to each split\n",
    "train_data = dataset_splits[\"train\"]\n",
    "intask_val_data = dataset_splits[\"intask_valid\"]\n",
    "posttask_val_data = dataset_splits[\"posttask_valid\"]\n",
    "test_data = dataset_splits[\"test\"]\n",
    "\n",
    "# pre-trained Model on imageNet \n",
    "resnet = MultilabelClassifier(n_classes_per_task[0])\n",
    "seen_classes = 0\n",
    "# initialize data to train on first task\n",
    "for task in range(len(tasks)):\n",
    "    train_data.choose_task(task)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    seen_classes += n_classes_per_task[task]\n",
    "    \n",
    "        \n",
    "    new_fc = nn.Linear(resnet.num_ftrs, seen_classes)\n",
    "    \n",
    "    for cl in range(seen_classes-n_classes_per_task[task]):\n",
    "        new_fc.weight[cl].data = resnet.fc.weight[cl].data\n",
    "            \n",
    "    resnet.fc = new_fc\n",
    "    resnet = resnet.to(device)\n",
    "    params_to_update = resnet.parameters()\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in resnet.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "    else:\n",
    "        for name,param in resnet.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                pass\n",
    "\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    \n",
    "    resnet = train_model(resnet, trainloader, criterion, optimizer_ft , seen_classes,5)\n",
    "\n",
    "# resnet = train_model(resnet, dataloader_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
