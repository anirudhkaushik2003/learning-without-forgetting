{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895121aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.0\n",
      "Torchvision Version:  0.14.0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "from iirc.datasets_loader import get_lifelong_datasets\n",
    "from iirc.definitions import PYTORCH, IIRC_SETUP\n",
    "from iirc.utils.download_cifar import download_extract_cifar100\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MultilabelJaccardIndex\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "\n",
    "from IIRC_CIFAR_HIERARCHY import classHierarchy\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54f5158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading CIFAR 100\n",
      "dataset downloaded\n",
      "extracting CIFAR 100\n",
      "dataset extracted\n"
     ]
    }
   ],
   "source": [
    "download_extract_cifar100(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2380c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "essential_transforms_fn = transforms.ToTensor()\n",
    "augmentation_transforms_fn = transforms.Compose([\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bdb847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iirc_cifar100\n",
      "Setup used: IIRC\n",
      "Using PyTorch\n",
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "dataset_splits, tasks, class_names_to_idx = \\\n",
    "    get_lifelong_datasets(dataset_name = \"iirc_cifar100\",\n",
    "                          dataset_root = \"../../data\", # the imagenet folder (where the train and val folders reside, or the parent directory of cifar-100-python folder\n",
    "                          setup = IIRC_SETUP,\n",
    "                          framework = PYTORCH,\n",
    "                          tasks_configuration_id = 0,\n",
    "                          essential_transforms_fn = essential_transforms_fn,\n",
    "                          augmentation_transforms_fn = augmentation_transforms_fn,\n",
    "                          joint = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0aea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tasks))\n",
    "n_classes_per_task = []\n",
    "for task in tasks:\n",
    "    n_classes_per_task.append(len(task))\n",
    "n_classes_per_task = np.array(n_classes_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f45387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "intask_valid\n",
      "posttask_valid\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# lifelong_datasets['train'].choose_task(2)\n",
    "# print(list(zip(*lifelong_datasets['train']))[1])\n",
    "for i in dataset_splits:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e58594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a pretrained model (imageNet)\n",
    "model_name = \"resnet\" #choosing alexnet since it is \"relatively\" easy to train\n",
    "# model_name = \"squeezenet\" # changed to squeezeNet since it gets same acc as alex but smaller\n",
    "num_classes = 9 # in cifar100\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "num_epochs = 14\n",
    "\n",
    "feature_extract = False #set to false so we can finetune entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81089e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, criterion, optimizer, num_classes, num_epochs=5 ):\n",
    "    since = time.time() # including this just because\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "                \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # iterate over data\n",
    "        for inputs,label1,label2 in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "            label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "            label1 = label1.to(torch.float32)\n",
    "            label1 = label1.to(device)\n",
    "#             label2 = label2.to(device)\n",
    "\n",
    "\n",
    "            #empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "#             print(label1.dtype, outputs.dtype)\n",
    "#             print(outputs,label1)\n",
    "            loss = criterion(outputs, label1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "#             running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(\"len dataset = \",len(trainloader.dataset))\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "        print('{} Loss: {:.4f}'.format('train', epoch_loss))\n",
    "\n",
    "        print()\n",
    "        test_loader(model, testloader, num_classes, mode=0)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2405003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03ae5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilabelClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        self.model_wo_fc = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.num_ftrs = self.resnet.fc.in_features\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_wo_fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad6e52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes):\n",
    "    model_ft = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b14b6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,testloader,num_classes,mode=0):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    JS = MultilabelJaccardIndex(num_labels = int(num_classes), average='weighted')\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(testloader):\n",
    "            if i > 10:\n",
    "                break\n",
    "            images, label1,label2 = data\n",
    "            # since subclass labels are introduced after their corresponding superclass labels,\n",
    "            # in case we encounter a subclass label, we can assume it's superclass label has already been introduced\n",
    "            # or that it's superclass does not exist\n",
    "            if label1 in classHierarchy or label1 in classHierarchy.values(): #if subclass has superclass or is superclass\n",
    "                if label1 in classHierarchy.values(): # if label is superclass label\n",
    "                    label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "                    \n",
    "                    label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "                    label1 = label1.to(torch.int32)\n",
    "\n",
    "                    label = label1\n",
    "                else: # if label is subclass and has superclass\n",
    "                    label2 = label1\n",
    "                    label1 = classHierarchy[label1]\n",
    "                    \n",
    "                    label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "                    label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "                    label1 = label1.to(torch.int32)\n",
    "\n",
    "                    label2 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label2]))\n",
    "                    label2 = F.one_hot(label2, num_classes=num_classes)\n",
    "                    label2 = label2.to(torch.int32)\n",
    "\n",
    "                    label = label1 + label2\n",
    "                    \n",
    "            else: # subclass has no superclass\n",
    "                label1 = torch.from_numpy(np.array([class_names_to_idx[i] for i in label1]))\n",
    "                label1 = F.one_hot(label1, num_classes=num_classes)\n",
    "                label1 = label1.to(torch.int32)\n",
    "\n",
    "                label = label1\n",
    "\n",
    "            label = label.to(torch.int32)\n",
    "            outputs = model(images) # sigmoidless activation\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             print(predicted)\n",
    "\n",
    "            correct += JS(outputs, label)\n",
    "#             correct += (predicted == label).sum().item()\n",
    "#             correct /= batch_size\n",
    "\n",
    "#             print(predicted,\"\\n\",label)\n",
    "        if mode == 0:\n",
    "            print(f\"In-task validation accuracy: {100 * correct // 11} %\")\n",
    "        elif mode == 1:\n",
    "            print(f\"Post-task validation accuracy: {100 * correct // 11} %\")\n",
    "        elif mode == 2:\n",
    "            print(f\"Final Test accuracy: {100 * correct // 11} %\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8188f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.3900\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2995\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2749\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2564\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2405\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2285\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2193\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2086\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.2020\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1957\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1888\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1830\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1779\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1743\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  8160\n",
      "train Loss: 0.1698\n",
      "\n",
      "Training complete in 10m 46s\n",
      "In-task validation accuracy: 6.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.3248\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.1442\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.1198\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.1029\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0961\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0925\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0870\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0844\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0839\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0807\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0769\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0788\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0770\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0735\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0714\n",
      "\n",
      "Training complete in 2m 33s\n",
      "In-task validation accuracy: 10.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.2573\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.1041\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0860\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0770\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0719\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0690\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0652\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0626\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0592\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0566\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0555\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0546\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0538\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0520\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  2160\n",
      "train Loss: 0.0516\n",
      "\n",
      "Training complete in 2m 59s\n",
      "In-task validation accuracy: 7.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.2615\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0984\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0749\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0678\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0618\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0584\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0583\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0539\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0510\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0499\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0466\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0463\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0473\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0440\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0436\n",
      "\n",
      "Training complete in 2m 27s\n",
      "In-task validation accuracy: 6.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.2725\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0993\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0785\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0700\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0656\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0600\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0601\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0553\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0534\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0521\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0506\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0500\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0478\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0469\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0464\n",
      "\n",
      "Training complete in 2m 13s\n",
      "In-task validation accuracy: 7.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.2358\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0820\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0652\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0574\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0514\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0494\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0454\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0441\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0425\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0428\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0407\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0403\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0391\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0383\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0364\n",
      "\n",
      "Training complete in 2m 19s\n",
      "In-task validation accuracy: 10.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.2441\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0843\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0662\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0584\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0547\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0514\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0494\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0476\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0461\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0454\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0453\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0437\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0416\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0421\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0412\n",
      "\n",
      "Training complete in 2m 27s\n",
      "In-task validation accuracy: 7.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.2333\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0754\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0581\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0507\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0452\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0431\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0407\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0397\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0378\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0378\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0348\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0349\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0352\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0336\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0325\n",
      "\n",
      "Training complete in 2m 18s\n",
      "In-task validation accuracy: 9.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.2279\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0756\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0585\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0519\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0478\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0448\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0438\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0417\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0408\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0399\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0391\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0388\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0373\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0360\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0361\n",
      "\n",
      "Training complete in 2m 27s\n",
      "In-task validation accuracy: 7.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.2208\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0728\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0574\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0498\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0468\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0437\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0414\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0402\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0386\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0384\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0381\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0363\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0358\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1760\n",
      "train Loss: 0.0359\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataset =  1760\n",
      "train Loss: 0.0350\n",
      "\n",
      "Training complete in 2m 20s\n",
      "In-task validation accuracy: 5.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.1917\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0568\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0443\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0390\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0355\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0340\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0319\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0311\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0314\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0288\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0281\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0277\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0274\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0270\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  2080\n",
      "train Loss: 0.0264\n",
      "\n",
      "Training complete in 2m 53s\n",
      "In-task validation accuracy: 5.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.2176\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0625\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0474\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0403\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0358\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0333\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0328\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0301\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0299\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0289\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0274\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0278\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0265\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0266\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0258\n",
      "\n",
      "Training complete in 2m 13s\n",
      "In-task validation accuracy: 5.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.1680\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0493\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0372\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0331\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0297\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0281\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0267\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0269\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0258\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0247\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0240\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0237\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0228\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0229\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  2320\n",
      "train Loss: 0.0222\n",
      "\n",
      "Training complete in 3m 11s\n",
      "In-task validation accuracy: 6.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.2000\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0588\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0449\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0391\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0366\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0340\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0324\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0314\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0308\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0297\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0295\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0291\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0289\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0279\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1840\n",
      "train Loss: 0.0274\n",
      "\n",
      "Training complete in 2m 30s\n",
      "In-task validation accuracy: 6.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.2194\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0640\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0472\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0419\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0373\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0346\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0332\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0319\n",
      "\n",
      "Epoch 9/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0309\n",
      "\n",
      "Epoch 10/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0300\n",
      "\n",
      "Epoch 11/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0288\n",
      "\n",
      "Epoch 12/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0284\n",
      "\n",
      "Epoch 13/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0273\n",
      "\n",
      "Epoch 14/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0278\n",
      "\n",
      "Epoch 15/15\n",
      "len dataset =  1600\n",
      "train Loss: 0.0270\n",
      "\n",
      "Training complete in 2m 12s\n",
      "In-task validation accuracy: 6.0 %\n",
      "Epoch 1/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.2139\n",
      "\n",
      "Epoch 2/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0571\n",
      "\n",
      "Epoch 3/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0436\n",
      "\n",
      "Epoch 4/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0372\n",
      "\n",
      "Epoch 5/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0337\n",
      "\n",
      "Epoch 6/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0310\n",
      "\n",
      "Epoch 7/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0299\n",
      "\n",
      "Epoch 8/15\n",
      "len dataset =  1680\n",
      "train Loss: 0.0284\n",
      "\n",
      "Epoch 9/15\n"
     ]
    }
   ],
   "source": [
    "# Setup \n",
    "# BCE loss for multi-label classification\n",
    "# sigmoid activation after FC layer \n",
    "# everything above 0.5 is a predicted label\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # as output is sigmoidless\n",
    "\n",
    "# get dataset corresponding to each split\n",
    "train_data = dataset_splits[\"train\"]\n",
    "intask_val_data = dataset_splits[\"intask_valid\"]\n",
    "posttask_val_data = dataset_splits[\"posttask_valid\"]\n",
    "test_data = dataset_splits[\"test\"]\n",
    "\n",
    "# pre-trained Model on imageNet \n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "seen_classes = 0\n",
    "# initialize data to train on first task\n",
    "for task in range(len(tasks)):\n",
    "    train_data.choose_task(task)\n",
    "    intask_val_data.choose_task(task)\n",
    "    posttask_val_data.choose_task(task)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    InTask_valloader = torch.utils.data.DataLoader(intask_val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    PostTask_valloader = torch.utils.data.DataLoader(posttask_val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    seen_classes += n_classes_per_task[task]\n",
    "    \n",
    "        \n",
    "    new_fc = nn.Linear(2048, seen_classes)\n",
    "    \n",
    "    for cl in range(seen_classes-n_classes_per_task[task]):\n",
    "        new_fc.weight[cl].data = resnet.fc.weight[cl].data\n",
    "            \n",
    "    resnet.fc = new_fc\n",
    "    resnet = resnet.to(device)\n",
    "    params_to_update = resnet.parameters()\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in resnet.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "    else:\n",
    "        for name,param in resnet.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                pass\n",
    "\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "    \n",
    "    resnet = train_model(resnet, trainloader, InTask_valloader, criterion, optimizer_ft , seen_classes,num_epochs)\n",
    "    test_model(resnet, PosTask_valloader,seen_classes, mode=1)\n",
    "\n",
    "# resnet = train_model(resnet, dataloader_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ec8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/resnet_IIRC.pth\"\n",
    "torch.save(resnet.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
