{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f4b1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.0\n",
      "Torchvision Version:  0.14.0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "from iirc.datasets_loader import get_lifelong_datasets\n",
    "from iirc.definitions import PYTORCH, IIRC_SETUP\n",
    "from iirc.utils.download_cifar import download_extract_cifar100\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torchvision.transforms as tt\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MultilabelJaccardIndex\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from metrics import jaccard_sim, modified_jaccard_sim, strict_accuracy, recall\n",
    "from resnetcifar import ResNetCIFAR\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n",
    "\n",
    "from IIRC_CIFAR_HIERARCHY import classHierarchy\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2194ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting CIFAR 100\n",
      "dataset extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/anirudhkaushik/miniconda3/envs/pytorch/lib/python3.10/site-packages/iirc/utils/download_cifar.py:15: UserWarning: ../../data/cifar-100-python.tar.gz already exists, the dataset won't be download\n",
      "  warnings.warn(f\"{target_path} already exists, the dataset won't be download\")\n"
     ]
    }
   ],
   "source": [
    "download_extract_cifar100(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ae20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_transforms_fn = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)),\n",
    "])\n",
    "augmentation_transforms_fn = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(32, padding=4),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42eb54d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating iirc_cifar100\n",
      "Setup used: IIRC\n",
      "Using PyTorch\n",
      "Dataset created\n"
     ]
    }
   ],
   "source": [
    "dataset_splits, tasks, class_names_to_idx = \\\n",
    "    get_lifelong_datasets(dataset_name = \"iirc_cifar100\",\n",
    "                          dataset_root = \"../../data\", # the imagenet folder (where the train and val folders reside, or the parent directory of cifar-100-python folder\n",
    "                          setup = IIRC_SETUP,\n",
    "                          framework = PYTORCH,\n",
    "                          tasks_configuration_id = 0,\n",
    "                          essential_transforms_fn = essential_transforms_fn,\n",
    "                          augmentation_transforms_fn = augmentation_transforms_fn,\n",
    "                          joint = True\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91edc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tasks))\n",
    "n_classes_per_task = []\n",
    "for task in tasks:\n",
    "    n_classes_per_task.append(len(task))\n",
    "n_classes_per_task = np.array(n_classes_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d580e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "intask_valid\n",
      "posttask_valid\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# lifelong_datasets['train'].choose_task(2)\n",
    "# print(list(zip(*lifelong_datasets['train']))[1])\n",
    "for i in dataset_splits:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4d9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a pretrained model (imageNet)\n",
    "model_name = \"resnet\" #choosing alexnet since it is \"relatively\" easy to train\n",
    "# model_name = \"squeezenet\" # changed to squeezeNet since it gets same acc as alex but smaller\n",
    "num_classes = 9 # in cifar100\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "num_epochs = 14\n",
    "\n",
    "feature_extract = False #set to false so we can finetune entire model\n",
    "\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2156c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, criterion, optimizer, num_classes, task_id, num_epochs=5, temperature=1 ):\n",
    "    since = time.time() # including this just because\n",
    "    \n",
    "    if task_id == 0:\n",
    "        num_epochs *= 2 # train for 2x num_epochs for the first task as compared to other tasks\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        \n",
    "                \n",
    "        running_loss = 0.0\n",
    "        data_len = 0\n",
    "        # iterate over data\n",
    "        for images,label1,label2 in tqdm(trainloader):\n",
    "            images = images.to(device)\n",
    "            label1 = one_hot_encode_labels(label1, num_classes=num_classes, gpu=True)\n",
    "#             label2 = label2.to(device)\n",
    "\n",
    "\n",
    "            #empty the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, _ = model(images)\n",
    "            \n",
    "            offset_1, offset_2 = compute_offsets(task_id)\n",
    "            outputs = outputs[:, :offset_2]\n",
    "            predictions = outputs > 0.0\n",
    "            \n",
    "            loss = criterion(outputs/temperature, label1)\n",
    "            loss.backward()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_metrics['jaccard_sim'] += jaccard_sim(predictions.to(torch.int32), label1.to(torch.int32)) * images.shape[0]\n",
    "            train_metrics['modified_jaccard_sim'] += modified_jaccard_sim(predictions.to(torch.int32), label1.to(torch.int32)) * images.shape[0]\n",
    "            train_metrics['strict_acc'] += strict_accuracy(predictions.to(torch.int32), label1.to(torch.int32)) * images.shape[0]\n",
    "            train_metrics['recall'] += recall(predictions.to(torch.int32), label1.to(torch.int64)) * images.shape[0]\n",
    "            data_len += images.shape[0]\n",
    "            \n",
    "        train_metrics['jaccard_sim'] /= data_len  \n",
    "        train_metrics['modified_jaccard_sim'] /= data_len  \n",
    "        train_metrics['strict_acc'] /= data_len  \n",
    "        train_metrics['recall'] /= data_len \n",
    "        \n",
    "        \n",
    "                \n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(\"len dataset = \",len(trainloader.dataset))\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "        print('{} Loss: {:.4f}'.format('train', epoch_loss))\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        print(f\"===Training Scores===\")\n",
    "        print(f\"JS: {train_metrics['jaccard_sim']} \")\n",
    "        print(f\"modified JS: {train_metrics['modified_jaccard_sim']} \")\n",
    "        print(f\"strict accuracy: {train_metrics['strict_acc']} \")\n",
    "        print(f\"Recall: {train_metrics['recall']} \")\n",
    "        test_model(model, testloader, num_classes, task_id, mode=0)\n",
    "        model = model.to(device)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59383894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52188c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(label, num_classes, gpu=False):\n",
    "    \n",
    "    label = torch.from_numpy(np.array([class_names_to_idx[i] for i in label]))\n",
    "    label = F.one_hot(label, num_classes=num_classes)\n",
    "    label = label.to(torch.float32)\n",
    "    if gpu:\n",
    "        label = label.to(device)\n",
    "                       \n",
    "    return label\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09a23d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offsets(task):\n",
    "        offset1 = int(sum(n_classes_per_task[:task]))\n",
    "        offset2 = int(sum(n_classes_per_task[:task + 1]))\n",
    "        return offset1, offset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a54114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,testloader, num_classes, task_id, mode=0):\n",
    "    with torch.no_grad():\n",
    "        print(f\"Begining Testing on task {task_id}\")\n",
    "        data_len = 0 \n",
    "        \n",
    "        for i,data in enumerate(tqdm(testloader)):\n",
    "            images, label1,label2 = data\n",
    "            images = images.to(device)\n",
    "            # since subclass labels are introduced after their corresponding superclass labels,\n",
    "            # in case we encounter a subclass label, we can assume it's superclass label has already been introduced\n",
    "            # or that it's superclass does not exist\n",
    "            if not any([class_names_to_idx[j] for j in label1] < seen_classes):\n",
    "                continue\n",
    "            if label1 in classHierarchy or label1 in classHierarchy.values(): #if subclass has superclass or is superclass\n",
    "                if label1 in classHierarchy.values(): # if label is superclass label\n",
    "                    label1 = one_hot_encode_labels(label1, num_classes=num_classes)\n",
    "                    label = label1\n",
    "                else: # if label is subclass and has superclass\n",
    "                    label2 = label1\n",
    "                    label1 = classHierarchy[label1]\n",
    "                    \n",
    "                    label1 = one_hot_encode_labels(label1, num_classes=num_classes)\n",
    "\n",
    "                    label2 = one_hot_encode_labels(label2, num_classes=num_classes)\n",
    "\n",
    "                    label = label1 | label2\n",
    "                    \n",
    "            else: # subclass has no superclass\n",
    "                label1 = one_hot_encode_labels(label1, num_classes=num_classes)\n",
    "                label = label1\n",
    "\n",
    "            label = label.to(torch.int32)\n",
    "            outputs, _ = model(images) # sigmoidless \n",
    "            outputs = outputs.detach().cpu()\n",
    "            offset_1, offset_2 = compute_offsets(task_id)\n",
    "            outputs = outputs[:, :offset_2]\n",
    "            predictions = outputs > 0.0\n",
    "#             print(predicted)\n",
    "\n",
    "            valid_metrics['jaccard_sim'] += jaccard_sim(predictions.to(torch.int32), label.to(torch.int32)) * images.shape[0]\n",
    "            valid_metrics['modified_jaccard_sim'] += modified_jaccard_sim(predictions.to(torch.int32), label.to(torch.int32)) * images.shape[0]\n",
    "            valid_metrics['strict_acc'] += strict_accuracy(predictions.to(torch.int32), label.to(torch.int32)) * images.shape[0]\n",
    "            valid_metrics['recall'] += recall(predictions.to(torch.int32), label.to(torch.int64)) * images.shape[0]\n",
    "            data_len += images.shape[0]\n",
    "            \n",
    "        valid_metrics['jaccard_sim'] /= data_len  \n",
    "        valid_metrics['modified_jaccard_sim'] /= data_len  \n",
    "        valid_metrics['strict_acc'] /= data_len  \n",
    "        valid_metrics['recall'] /= data_len  \n",
    "            \n",
    "        \n",
    "            \n",
    "#             correct += (predicted == label).sum().item()\n",
    "#             correct /= batch_size\n",
    "\n",
    "#             print(preds,label)\n",
    "        if mode == 0:\n",
    "            print(\"===In-task validation===\")\n",
    "            print(f\"JS: {valid_metrics['jaccard_sim']} \")\n",
    "            print(f\"modified JS: {valid_metrics['modified_jaccard_sim']} \")\n",
    "            print(f\"strict accuracy: {valid_metrics['strict_acc']} \")\n",
    "            print(f\"Recall: {valid_metrics['recall']} \")\n",
    "            \n",
    "            \n",
    "        elif mode == 1:\n",
    "            print(\"===Post-task validation===\")\n",
    "            print(f\"JS: {valid_metrics['jaccard_sim']} \")\n",
    "            print(f\"modified JS: {valid_metrics['modified_jaccard_sim']} \")\n",
    "            print(f\"strict accuracy: {valid_metrics['strict_acc']} \")\n",
    "            print(f\"Recall: {valid_metrics['recall']} \")\n",
    "            \n",
    "        elif mode == 2:\n",
    "            print(\"===Final Test Scores===\")\n",
    "            print(f\"JS: {valid_metrics['jaccard_sim']} \")\n",
    "            print(f\"modified JS: {valid_metrics['modified_jaccard_sim']} \")\n",
    "            print(f\"strict accuracy: {valid_metrics['strict_acc']} \")\n",
    "            print(f\"Recall: {valid_metrics['recall']} \")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f18d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begining Training on Task 1\n",
      "Epoch 1/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:15<00:00, 39.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataset =  40000\n",
      "train Loss: 0.0106\n",
      "\n",
      "===Training Scores===\n",
      "JS: 0.008307987455790863 \n",
      "modified JS: 0.008275249048990373 \n",
      "strict accuracy: 0.00825 \n",
      "Recall: 7.6451 \n",
      "Begining Testing on task 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:14<00:00, 89.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===In-task validation===\n",
      "JS: 0.0214 \n",
      "modified JS: 0.0214 \n",
      "strict accuracy: 0.0214 \n",
      "Recall: 5.2512 \n",
      "Epoch 2/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [03:45<00:00, 44.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len dataset =  40000\n",
      "train Loss: 0.0095\n",
      "\n",
      "===Training Scores===\n",
      "JS: 0.02774604103326808 \n",
      "modified JS: 0.02767173465921096 \n",
      "strict accuracy: 0.027600206249999995 \n",
      "Recall: 4.7342911275 \n",
      "Begining Testing on task 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:14<00:00, 85.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===In-task validation===\n",
      "JS: 0.051804279999999994 \n",
      "modified JS: 0.05150427999999999 \n",
      "strict accuracy: 0.05120427999999999 \n",
      "Recall: 11.69145024 \n",
      "Epoch 3/28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 9301/10000 [03:21<00:14, 48.42it/s]"
     ]
    }
   ],
   "source": [
    "# Setup \n",
    "# BCE loss for multi-label classification\n",
    "# sigmoid activation after FC layer \n",
    "# everything above 0.5 is a predicted label\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\") # as output is sigmoidless\n",
    "\n",
    "# get dataset corresponding to each split\n",
    "train_data = dataset_splits[\"train\"]\n",
    "intask_val_data = dataset_splits[\"intask_valid\"]\n",
    "posttask_val_data = dataset_splits[\"posttask_valid\"]\n",
    "test_data = dataset_splits[\"test\"]\n",
    "\n",
    "# pre-trained Model on imageNet \n",
    "# resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "seen_classes = 0\n",
    "seen_classes_list = []\n",
    "output_layer_size = int(sum(n_classes_per_task))\n",
    "\n",
    "# additional network features\n",
    "temperature = 1.0\n",
    "weight_decay = 1e-5\n",
    "\n",
    "\n",
    "resnet = ResNetCIFAR(num_classes=output_layer_size, num_layers=20 )\n",
    "resnet = resnet.to(device)\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# scheduler # TO DO\n",
    "\n",
    "# resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "# initialize data to train on first task\n",
    "for task_id, task in enumerate(tasks):\n",
    "    train_metrics = {\"jaccard_sim\" : 0, \"modified_jaccard_sim\" : 0, \"strict_acc\" : 0, \"recall\" : 0}\n",
    "    valid_metrics = {\"jaccard_sim\" : 0, \"modified_jaccard_sim\" : 0, \"strict_acc\" : 0, \"recall\" : 0}\n",
    "    train_data.choose_task(task_id)\n",
    "    intask_val_data.choose_task(task_id)\n",
    "    posttask_val_data.choose_task(task_id)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    InTask_valloader = torch.utils.data.DataLoader(intask_val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    PostTask_valloader = torch.utils.data.DataLoader(posttask_val_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    seen_classes += n_classes_per_task[task_id]\n",
    "    seen_classes_list = list(set(seen_classes_list) | set(task))\n",
    "            \n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        print(f\"Training on multiple gps ({ngpu})\")    \n",
    "        resnet = nn.DataParallel(resnet,list(range(ngpu)))\n",
    "            \n",
    "    print(f\"Begining Training on Task {task_id+1}\")\n",
    "    resnet = train_model(resnet, trainloader, InTask_valloader, criterion, optimizer, seen_classes,task_id, num_epochs)\n",
    "    test_model(resnet, InTask_valloader,seen_classes, task_id, mode=1)\n",
    "\n",
    "# resnet = train_model(resnet, dataloader_dict, criterion, optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b07933",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/resnet50_JOINT.pth\"\n",
    "torch.save(resnet.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
